{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predict EE (Enantionmeric Excesses)\n",
    "\n",
    "*Zhongying Ru  zilla_ru@zju.edu.cn  Apr.29th*\n",
    "\n",
    "\n",
    "### Input\n",
    "\n",
    "SMILES: Simplified Molecular Input Line Entry System\n",
    "\n",
    "- **SM1**\n",
    "- **SM2**\n",
    "- SM_metal (optional)\n",
    "- SM_ligand\n",
    "- SM_solvent (optional)\n",
    "\n",
    "### Pipeline\n",
    "\n",
    "1. SMILES string -- RDkit-> molecule graph  --GNN-> vector representation\n",
    "\n",
    "2. concatenate the vector representations of the 5 input variables\n",
    "\n",
    "3. fully-connected layers or CNNs (*todo*)\n",
    "\n",
    "\n",
    "### Output\n",
    "\n",
    "- predicted EE\n",
    "\n",
    "### Loss Fuction\n",
    "\n",
    "#### predict in an end-to-end manner\n",
    "Measure the sum of difference from the predicted EE to the true EE of **training samples**\n",
    "\n",
    "### Train/Valid/Test set division\n",
    "\n",
    "randomly divide as 8:1:1 (approximately)\n",
    "or\n",
    "use the predefined train/test files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare packages\n",
    "\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import dgl\n",
    "from rdkit import Chem\n",
    "from torch.utils.data import DataLoader\n",
    "from dgllife.utils import mol_to_complete_graph, mol_to_bigraph\n",
    "from dgllife.utils import atom_type_one_hot\n",
    "from dgllife.utils import atom_degree_one_hot\n",
    "from dgllife.utils import atom_formal_charge\n",
    "from dgllife.utils import atom_num_radical_electrons\n",
    "from dgllife.utils import atom_hybridization_one_hot\n",
    "from dgllife.utils import atom_total_num_H_one_hot\n",
    "from dgllife.utils import CanonicalAtomFeaturizer\n",
    "from dgllife.utils import CanonicalBondFeaturizer\n",
    "from dgllife.utils import ConcatFeaturizer\n",
    "from dgllife.utils import BaseAtomFeaturizer\n",
    "from dgllife.utils import BaseBondFeaturizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "import csv\n",
    "from itertools import islice\n",
    "train_file = 'data/train_no_metal.csv'\n",
    "test_file = 'data/test_no_metal.csv'\n",
    "mol_id_dict_filename = 'data/n0_metal_sm_to_id.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess: map molecule SMILES to id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sm_cnt = 407, sm1_cnt = 184, sm2_cnt = 43, metal_cnt = 0, ligand_cnt = 144, solvent_cnt = 36\n"
     ]
    }
   ],
   "source": [
    "# load train & test files\n",
    "# format (SM1, SM2, metal, ligand, solvent, ee)\n",
    "\n",
    "sm_to_id_dict = dict()\n",
    "sm_cnt, sm1_cnt, sm2_cnt, met_cnt, lig_cnt, sol_cnt= 0, 0, 0, 0, 0, 0\n",
    "\n",
    "def build_mol_dict(fname):\n",
    "    global sm_to_id_dict, sm_cnt, sm1_cnt, sm2_cnt, met_cnt, lig_cnt, sol_cnt\n",
    "    id_id_ee_list = list()\n",
    "    with open(fname, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        \n",
    "        for row in islice(reader, 1, None):  # if the csv has a header , skip the 1st row\n",
    "            sm1, sm2, met, lig, sol, ee = row\n",
    "            if sm1 not in sm_to_id_dict:\n",
    "                sm_to_id_dict[sm1] = sm_cnt\n",
    "                sm_cnt+=1\n",
    "                sm1_cnt+=1\n",
    "            if sm2 not in sm_to_id_dict:\n",
    "                sm_to_id_dict[sm2] = sm_cnt\n",
    "                sm_cnt+=1\n",
    "                sm2_cnt+=1\n",
    "            if met != '' and met not in sm_to_id_dict:\n",
    "                sm_to_id_dict[met] = sm_cnt\n",
    "                sm_cnt+=1\n",
    "                met_cnt+=1\n",
    "            if lig not in sm_to_id_dict:\n",
    "                sm_to_id_dict[lig] = sm_cnt\n",
    "                sm_cnt+=1\n",
    "                lig_cnt+=1\n",
    "            if sol not in sm_to_id_dict:\n",
    "                sm_to_id_dict[sol] = sm_cnt\n",
    "                sm_cnt+=1\n",
    "                sol_cnt+=1\n",
    "            if met == '':\n",
    "                met = -1\n",
    "            else:\n",
    "                met = sm_to_id_dict[met]\n",
    "            id_id_ee_list.append((sm_to_id_dict[sm1], sm_to_id_dict[sm2], met, sm_to_id_dict[lig], sm_to_id_dict[sol], float(ee)))\n",
    "        np.save(fname[:-4]+'_id.npy', id_id_ee_list)\n",
    "            \n",
    "build_mol_dict(train_file)\n",
    "build_mol_dict(test_file)\n",
    "print(f'sm_cnt = {sm_cnt}, sm1_cnt = {sm1_cnt}, sm2_cnt = {sm2_cnt}, \\\n",
    "metal_cnt = {met_cnt}, ligand_cnt = {lig_cnt}, solvent_cnt = {sol_cnt}')\n",
    "\n",
    "# save `sm_to_id_dict` to file\n",
    "np.save(mol_id_dict_filename, sm_to_id_dict)\n",
    "# dict = np.load(mol_id_dict_filename).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_metal_emb = False\n",
    "use_ligand_emb = True\n",
    "use_solvent_emb = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'dgllife.model.gnn.attentivefp' from '/Users/zilla/py/anaconda3/envs/dgl_lifesci/lib/python3.6/site-packages/dgllife/model/gnn/attentivefp.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input: SMILES strings of molecules\n",
    "# intermediate output: molecule representation\n",
    "import dgllife.model.gnn.attentivefp as AFP\n",
    "AFP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an AttentiveFP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AFP_EE_Predictor(nn.Module):\n",
    "    \"\"\"\n",
    "    an end-to-end model based on AttentiveFP for regression\n",
    "\n",
    "    AttentiveFP is introduced in\n",
    "    `Pushing the Boundaries of Molecular Representation for Drug Discovery with the Graph\n",
    "    Attention Mechanism. <https://www.ncbi.nlm.nih.gov/pubmed/31408336>`__\n",
    "\n",
    "    AttentiveFP Parameters\n",
    "    ----------\n",
    "    node_feat_size : int\n",
    "        Size for the input node features.\n",
    "    edge_feat_size : int\n",
    "        Size for the input edge features.\n",
    "    num_layers : int\n",
    "        Number of GNN layers. Default to 2.\n",
    "    num_timesteps : int\n",
    "        Times of updating the graph representations with GRU. Default to 2.\n",
    "    graph_feat_size : int\n",
    "        Size for the learned graph representations. Default to 200.\n",
    "        \n",
    "    \n",
    "    Task-oriented Predictor Parameters\n",
    "    ----------\n",
    "    pred_input_size : int\n",
    "        depend on graph_feat_size and the concat manner\n",
    "    n_tasks : int\n",
    "        Number of tasks, which is also the output size. Default to 1.\n",
    "    dropout : float\n",
    "        Probability for performing the dropout. Default to 0.\n",
    "    \"\"\"\n",
    "    class EE_Predictor(nn.Module):\n",
    "        def __init__(self,\n",
    "                     input_size,\n",
    "                     n_task=1,\n",
    "                     dropout=0.):\n",
    "            self.in_size = input_size\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(input_size, n_tasks)\n",
    "            )\n",
    "        def forward(self, g_feats, samples): # , sm_id_dict\n",
    "            # graph feat -> sample feat\n",
    "            sample_feats = np.zeros(shape=(len(samples), self.in_size)) #  dtype=float\n",
    "            i = 0\n",
    "            for sm1_id, sm2_id, met_id, lig_id, sol_id in samples:\n",
    "                sample_feat = np.concatenate((g_feats[sm1_id], \n",
    "                                              g_feats[sm2_id], \n",
    "                                              g_feats[met_id], \n",
    "                                              g_feats[lig_id], \n",
    "                                              g_feats[sol_id]),\n",
    "                                             axis=0) if met_id!=-1 else np.concatenate((g_feats[sm1_id],\n",
    "                                                                                        g_feats[sm2_id],\n",
    "                                                                                        g_feats[lig_id], \n",
    "                                                                                        g_feats[sol_id]),\n",
    "                                                                                       axis=0)\n",
    "                sample_feats[i] = sample_feat\n",
    "                i+=1\n",
    "                \n",
    "            return self.layer(sample_feats)\n",
    "        \n",
    "    def __init__(self,\n",
    "                 node_feat_size,\n",
    "                 edge_feat_size,\n",
    "                 num_layers=2,\n",
    "                 num_timesteps=2,\n",
    "                 graph_feat_size=200,\n",
    "                 pred_input_size=800, # 4*200 - no metal, 5*200 - metal\n",
    "                 n_tasks=1,\n",
    "                 dropout=0.):\n",
    "        super(AttentiveFPPredictor, self).__init__()\n",
    "        \n",
    "        self.gnn = AttentiveFPGNN(node_feat_size=node_feat_size,\n",
    "                                  edge_feat_size=edge_feat_size,\n",
    "                                  num_layers=num_layers,\n",
    "                                  graph_feat_size=graph_feat_size,\n",
    "                                  dropout=dropout)\n",
    "        self.readout = AttentiveFPReadout(feat_size=graph_feat_size,\n",
    "                                          num_timesteps=num_timesteps,\n",
    "                                          dropout=dropout)\n",
    "        self.predict = EE_Predictor(pred_input_size)\n",
    "\n",
    "    def forward(self, g, samples, node_feats, edge_feats, get_node_weight=False):\n",
    "        \"\"\"Graph-level regression/soft classification.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        g : DGLGraph\n",
    "            DGLGraph for a batch of graphs.\n",
    "            Here, we treat all molecule graphs as a batch.\n",
    "            \n",
    "        node_feats : float32 tensor of shape (V, node_feat_size)\n",
    "            Input node features. V for the number of nodes.\n",
    "            \n",
    "        edge_feats : float32 tensor of shape (E, edge_feat_size)\n",
    "            Input edge features. E for the number of edges.\n",
    "            \n",
    "        get_node_weight : bool\n",
    "            Whether to get the weights of atoms during readout. Default to False.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float32 tensor of shape (G, n_tasks)\n",
    "            Prediction for the graphs in the batch. G for the number of graphs.\n",
    "            \n",
    "        node_weights : list of float32 tensor of shape (V, 1), optional\n",
    "            This is returned when ``get_node_weight`` is ``True``.\n",
    "            The list has a length ``num_timesteps`` and ``node_weights[i]``\n",
    "            gives the node weights in the i-th update.\n",
    "        \"\"\"\n",
    "        node_feats = self.gnn(g, node_feats, edge_feats)\n",
    "        if get_node_weight:\n",
    "            g_feats, node_weights = self.readout(g, node_feats, get_node_weight)\n",
    "            return self.predict(g_feats, samples), node_weights\n",
    "        else:\n",
    "            g_feats = self.readout(g, node_feats, get_node_weight)\n",
    "            return self.predict(g_feats, samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}